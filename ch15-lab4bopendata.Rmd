# Lab 4B; Open data & Getting data {#lab4bopendata}


```{r setup15, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      fig.width = 5, fig.height = 3)
```

## Packages
```{r}
library(tidyverse)
```


## Aims

This lesson is about getting data. We already saw in Chapter \@ref(lab4aimportingdata) that external data (contained in data files) can be read into R in a number of different ways. Here we focus on data that is not generated by yourself or available as a local data file. This chapter focusses on getting your hands on Open Data or externally available data. For a shortlist op popular Open Data sources see:

[Open Data](https://www.freecodecamp.org/news/https-medium-freecodecamp-org-best-free-open-data-sources-anyone-can-use-a65b514b0f2d/)

To get Open Data data into R you can use a number of methods.
Depending on the aim or nature of your problem or challenge you can either use: 
 
  1. Data in R packages
  1. Download files from a website or other internet source
  1. Repositories: Data-repos / Github
  1. Scientific Databases
  1. Data Science Contests (Kaggle)
  1. Amazon WebServices - S3 buckets
  1. Dedicated API's
  1. Through scraping of websites
  
## Open Science

Open Data is an an important prerequisite and concept contained within the Open Science framework. This framework shortly entails the following concepts: 

 - Information (meta data), data and analysis (code and tools) are combined and stored _together_
 - Freely available (under e.g. CC-BY-NC 4.0 LICENCE)
 - Share scientific information before and during study, not only when finished
 - Open during the whole process and in every step of doing science
 - Reproducibility is key
 - Share data, methods and code on open platforms (e.g. Github.com)

In this chapter we will go over an example for each of these methods.

## Data in R packages
  
### <mark>**EXERCISE 1 - base R `{datasets}`**</mark> {-} 

The R base installation comes with a package called `{datasets}`. This package is loaded when you start RStudio. 

A) Type `?datasets` in the Console.

B) List all datasets from the `{datasets}` package
```{r, echo=FALSE, results='hide', eval=FALSE}
data(package = "datasets")
```

C) Load the `Titanic` dataset from the `{datasets}` package
```{r, include=FALSE, eval=FALSE}
data(package = "datasets", "Titanic")
titanic <- Titanic %>% as_tibble
head(titanic, 3)
```

### <mark>**EXERCISE 2 - Access data included in R packages**</mark> {-}

Often, data is included in R packages for examples on how to use methods proposed in the package or to illustrate statistical principles.

Load the dataset "BostonHousing" from the `{mlbench}` package
```{r, include=FALSE, eval=FALSE}
library(mlbench)
## to load Boston Housing Dataset
data("BostonHousing", package = "mlbench")
```

### Data only packages {-}
Sometimes all a package is, is data. For example the full human genome sequence can be downloaded as a bioconductor package. Or some machine learning datasets from other packages are available as a datapackage.
Here we load the `wine` dataset from the `{rattle.data}` package.

```{r, eval=FALSE}
data(package = "rattle.data")
help(wine, package = "rattle.data")
```

Or the full standardized human genome with: 
We show 101 nucleotides from chromosome 1, location 30,000 - 30,100
```{r}
# BiocManager::install("BSgenome.Hsapiens.UCSC.hg38")
library(BSgenome.Hsapiens.UCSC.hg38)
BSgenome.Hsapiens.UCSC.hg38$chr1[30000:30100]
```

### The `wine` dataset
The `wine` dataset is an example dataset, used for teaching and discovering machine learning. We will come back to this dataset in Chapter \@ref(lab6assumptionsmodels)

### <mark>**EXERCISE 3; Wine**</mark> {-}

Load the wine dataset from the `{ratlle.data}` package
```{r}
data(package = "rattle.data", "wine")
```

### <mark>**EXERCISE 4: Exploring the `wine` data with PCA**</mark> {-}

When exploring data like the wine dataset that has numerous numeric features to decribe or classify an observation, we can perform a Principal Component Analysis (PCA).

A) Review a demo on PCA here: http://huboqiang.cn/2016/03/03/RscatterPlotPCA
Using the information from this demo, perform and plot a PCA analysis (plot only PC1 vs PC2 en PC1 vs PC3). Remember that for principal component analysis you best transform the data by scaling and centering.

```{r, include=FALSE}
names(wine)
pca_data <- wine[,c(-1)] %>% 
  log10(.) %>%
  stats::prcomp(.,
                center = TRUE,
                scale = TRUE) 

pca_data$x %>%  
  as_tibble() %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_bw() 
```

B) Add colour to the points for the `Type` variable {-}
Write a piece of code that generates the following graph
```{r, echo=FALSE, results='hide'}
labels <- wine[,1]
pca_data$x %>%  
  as_tibble() %>%
  mutate(Type = labels) %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(aes(colour = Type)) +
  theme_bw() 
```

--- EXERCISE END ---

## Download files from a website or other internet source

Many files are downloadable as tab-seperated or comma-separated values. Often these files are hosted on a website of open-data repository. They can be downloaded directly from the url.

Visit the following page:
[Meldingen openbare ruimte Utrecht](https://ckan.dataplatform.nl/dataset/mor-utrecht/resource/e16e9af3-8dbc-4f82-aecf-1c69b628c81d)

### Download from an url with `download.file()`
The code below downloads the `.csv` of the above webpage file to the `/data` dir.

```{r, eval=FALSE}
url <- "https://ckan.dataplatform.nl/dataset/7dc70520-1653-49f6-a251-c95939bb6962/resource/e16e9af3-8dbc-4f82-aecf-1c69b628c81d/download/meldingen2015open-data.csv"

download.file(url = url, destfile = here::here(
  "data",
  "meldingen2015open-data.csv"
))
``` 

### Loading this dataset from Utrecht, The Netherlands into R

```{r}
meldingen2015open_data <- read_csv(here::here(
  "data",
  "meldingen2015open-data.csv"
))
```

### How many and which notifications (main categories) per Utrecht area?

```{r, fig.width=9, fig.height=5}
names(meldingen2015open_data)

meldingen2015open_data %>%
  group_by(Hoofdcategorie, Wijk) %>%
  tally() %>%
  ggplot(aes(x = reorder(as_factor(Hoofdcategorie), n), y = n)) +
  geom_point(aes(colour = Wijk), position = "jitter") +
    coord_flip() +
  toolboxr::rotate_axis_labels("x", 90) +
  xlab(NULL) +
  ylab(NULL)
```

## Repositories: Data-repos

There are many (too many to mention all here) dedicated datasources in the form of data repositories. I will show three examples here: Google datasets / Amazon S3 Buckets and KAGGLE as valuable sources for data.

## Via Google Datasets

I entered the search terms "Canis lupus" AND "Europe"
One of the hits was this link:

[download link](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194711#sec010)

The Citation for the data:
```
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194711#sec010
```

### Exploring this dataset {-}
```{r}
wolves <- readxl::read_xlsx(
  here::here("data",
             "S1Table.xlsx")
) %>%
  as_tibble()

wolves
names(wolves)
```

### <mark>**Exercise 5 - Wolves**</mark> {-}

A) Plot the relationship between the variables "Gross domestic product per capita (USD)" and "Population estimate". 
B) Transform both axes to a log10 scale

C) Add a linear relationship (`geom_smooth`, straight line)

D) Add a colour for each "Country group"

E) Now plot a linear relationship (straight line) for each "Country group"

```{r, include=FALSE}
wolves %>% 
  ggplot(aes(
    x = `Population estimate` %>% log10,
    y = `Gross domestic product per capita (USD)` %>% log10
  )) +
  geom_point(aes(colour = `Country group`)) +
  geom_smooth(
    aes(colour = `Country group`,
        group = `Country group`),
    method = "lm") 
```

F) Calculate the land area per "Country group" and plot this against the population estimate, what is your conclusion, considering the previous graph and this graph?
```{r}
wolves %>%
  group_by(`Country group`) %>%
  summarize(total_surface = sum(`Land area (km2)`),
            total_pop = sum(`Population estimate`)) %>%
  ggplot(aes(x = total_surface,
             y = total_pop)) +
  geom_col(aes(fill = `Country group`))


```

## Scientific Databases

There are many dedicated databases fro scientific data.

For example https://doi.org/10.3334/ORNLDAAC/1275
How to read netCDF files into R:
http://geog.uoregon.edu/bartlein/courses/geog490/week04-netCDF.html

### <mark>**EXERCISE 6 - Annual averages for temperature**</mark> {-}

A) Load the above data from georg into R.
You need to manually download the netcdf file first
```{r, eval=FALSE}
library(ncdf4)
# url <- "http://geog.uoregon.edu/GeogR/data/raster/cru10min30_tmp.nc"
# download.file(url = url, destfile = "test.nc", mode = "wb")
ncin <- nc_open(
  here::here(
  "data",  
  "test.nc"))
ncin

```

B) Follow the tutorial at: http://geog.uoregon.edu/bartlein/courses/geog490/week04-netCDF.html. Read until (not including) paragraph: "4 Data frame-to-array conversion(rectangular to raster)"

C) Get the annual mean per monthfor all longitudes and latitudes, for the complete dataset, no missing values
```{r, include=FALSE, eval=FALSE}
lon <- ncvar_get(ncin,"lon")
nlon <- dim(lon)
head(lon)
lat <- ncvar_get(ncin,"lat")
nlat <- dim(lat)
head(lat)
# create dataframe -- reshape data
# matrix (nlon*nlat rows by 2 cols) of lons and lats
lonlat <- as.matrix(expand.grid(lon,lat))
dim(lonlat)
lonlat

dname <- "tmp"
# vector of `tmp` values
# get a single slice or layer (January)
tmp_array <- ncvar_get(ncin, "tmp")
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(tmp_array)
m <- 1
tmp_slice <- tmp_array[,,m]
# levelplot of the slice
library(lattice)
library(RColorBrewer)
grid <- expand.grid(lon=lon, lat=lat)

cutpts <- c(-50,-40,-30,-20,-10,0,10,20,30,40,50)
levelplot(tmp_slice ~ lon * lat, data=grid, at=cutpts, cuts=11, pretty=T, 
  col.regions=(rev(brewer.pal(10,"RdBu"))))


tmp_vec <- as.vector(tmp_slice)
length(tmp_vec)
tmp_df01 <- data.frame(cbind(lonlat, tmp_vec))
names(tmp_df01) <- c("lon","lat",paste(dname,as.character(m), sep="_"))
head(na.omit(tmp_df01), 10)

tmp_vec_long <- as.vector(tmp_array)
length(tmp_vec_long)

# get time
time <- ncvar_get(ncin,"time")
time
tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
# reshape the vector into a matrix
tmp_mat <- matrix(tmp_vec_long, nrow=nlon*nlat, ncol=nt)
dim(tmp_mat)

# create a dataframe
lonlat <- as.matrix(expand.grid(lon,lat))
tmp_df02 <- data.frame(cbind(lonlat,tmp_mat))
names(tmp_df02) <- c("lon","lat","tmpJan","tmpFeb","tmpMar","tmpApr","tmpMay","tmpJun",
  "tmpJul","tmpAug","tmpSep","tmpOct","tmpNov","tmpDec")
# options(width=96)
head(na.omit(tmp_df02, 20))
# get the annual mean and MTWA and MTCO
tmp_df02$mtwa <- apply(tmp_df02[3:14],1,max) # mtwa
tmp_df02$mtco <- apply(tmp_df02[3:14],1,min) # mtco
tmp_df02$mat <- apply(tmp_df02[3:14],1,mean) # annual (i.e. row) means
head(na.omit(tmp_df02))
tmp_df03 <- na.omit(tmp_df02)
head(tmp_df03)

```

D) What is the temperature pattern (make a graph) for lon = "4.75", lat =	"52.25"
You will need to use 'gather()' to reshape the data first to a stacked format
```{r, include=FALSE, eval=FALSE}
tmp_df03 %>%
  dplyr::filter(lon == "4.75", lat == "52.25") %>%
  gather(tmpJan:tmpDec, key = "month", value = "temp") %>%
  ggplot(aes(x = month, y = temp)) +
  geom_point() + toolboxr::rotate_axis_labels(axis = "x", angle = 45)


```

E) Review the chapter ["Factors"](https://r4ds.had.co.nz/factors.html) of "R for Data Science" and reorder the factor levels so that data per month follows the right order. Create a new graph, displaying the right order of months and corresponding temperature

**Tips:**
 
 - Maybe it is a good idea to strip the 'tmp' part from the month variable names
 - Use dplyr to create a new ordered factor for month.
 - create a vector with the proper month levels as your first step (these will be the new intended levels for the month variable)
 - use the dataframe `tmp_df03` from the step at for this question.
 - maybe use `forcats::fct_relevel()` in conjunction with `dplyr::mutate()`?
 
```{r, include=FALSE, eval=FALSE}
names(tmp_df03)
tmp_df03_new <- tmp_df03 %>%
  dplyr::filter(lon == "4.75", lat == "52.25") %>%
  gather(tmpJan:tmpDec, key = "month", value = "temp") %>%
  mutate(
    month = str_replace_all(
      string = month, 
      pattern = regex("tmp"),
      replacement = ""
  )) %>%
  mutate(
    month = forcats::fct_relevel(
      month, "Jan", "Feb", "Mar", "Apr", "May", "Jun", 
             "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
         ))

tmp_df03_new %>%  
ggplot(aes(x = month, y = temp)) +
  geom_point() + 
  toolboxr::rotate_axis_labels(axis = "x", angle = 45) 

levels(tmp_df03_new$month)

```

F) Where on the planet was this measured (tip: Google is you friend) - find the address. Remember, this is an annual average ...

## Opening a .json format
```{r}
## http://www.programmingr.com/examples/reading-json-data/
library(jsonlite)
# r web / r json - get json data from url
json_file <- "https://ckan.dataplatform.nl/dataset/fcd7fb40-e11d-4200-8d69-6aef6dbd8e0a/resource/79298c25-76fe-4e2a-b2ce-193944abdc89/download/panden-json.zip"

download.file(json_file, destfile = "json.zip")
unzip("json.zip")
data <- fromJSON("panden-json.json")

df <- data$features %>% as_tibble
df
```

## Data Science Contests (Kaggle)

Kaggle is a great resource for learning Data Science

### <mark>**Exercise 7; KAGGLE**</mark> {-} 

A) Create a Kaggle account
B) Go over the analysis at https://www.kaggle.com/kailex/education-languages-and-salary 
C) What is the most preferred programming language on Kaggle 

## Amazon WebServices - S3 buckets

Amazon has many open datasets. The NO-SQL S3 system of Amazon and an API are easy to use for getting this data. Some datasets are quite big, so getting them could take a while (around 5 minutes). A Spark backend is used to speed things up. Here, we explore a dataset called "New York City Taxi and Limousine Commission (TLC) Trip Record Data". This dataset is a record of movements and pick-up-and-drop-off timestamps for major Taxi-services in New York City.

<mark>**Downloading the data takes a while (at least 5 minutes, so be patient when executing the final step of the code below). To prevent downloading the data each time: save the final dataset to a csv at the end.**</mark>

```{r, eval=FALSE}
## get data from 
## Bike Share 
# https://s3.amazonaws.com/capitalbikeshare-data/index.html
# Amazon open data buckets
# https://registry.opendata.aws/
library(aws.s3)
library(sparklyr)
library(dplyr)

nyc_tlc_bucket <- get_bucket(bucket = "nyc-tlc")
key <- nyc_tlc_bucket[[3]]$Key
nyc_tlc_csv <- aws.s3::s3read_using(
  read_csv, 
  object = key, 
  bucket = "nyc-tlc")
```

### Enhance performace with Spark (through `{sparklyr}` in R)
Connecting you local Spark engine to an online resource can be quite difficult but will yield speed. So if you'are downloading big datasets, often, it might be worth the hassle.
For illustration puposes: [here you can find how to connect your RStudio Spark (via the R-package `{sparklyr}`) to Amazon AWS S3:]( https://spark.rstudio.com/guides/aws-s3/) 
I will not get into details on this during the course.

### <mark>**EXERCISE 8; Time span?**</mark> {-}
Try to figure out what time span the data covers. hink carefully about how you will answer this question. Write steps in psudo-code first and then try to write the corresponding R code that provides the answer. 

### <mark>**If you are looking for a dataset to use for your capstone project, the TLC dataset is a good candidate. Maybe use `{sparklyr}`, as mentioned above?**</mark> {-}

## Air quality data - AWS
Another example for getting Open Data from AWS Amazon. The OpenAQ data can be retrieved in full from AWS S3 buckets the same way as we retrieved the TLC trip data records above. The full AWS S3 OpenAQ S3 Bucket is located [here:](https://registry.opendata.aws/openaq/)  

However, because of the size of this dataset we will use a slimmed-down version of the data. We can obtain part of the OpenAQ data throught the R-package `{ropenaq}` which is hosted on CRAN and on [Github](https://github.com/ropensci/ropenaq) 

## To get the the 'daily' bucket 
```{r, eval=FALSE}
openaq_bucket <- get_bucket(bucket = "openaq-fetches")
key <- openaq_bucket[[3]]$Key
openaq_aws <- aws.s3::s3read_using(
  read_csv, 
  object = key, 
  bucket = "openaq-fetches",
  col_names = FALSE)
```

The full list of all OpenAQ data can be reviewed [here](https://openaq-fetches.s3.amazonaws.com/index.html)

### Using the `{ropenaq}` package from [ROpenSci](https://ropensci.org/). This example was reproduced [from](https://docs.ropensci.org/ropenaq/articles/ropenaq.html). Execute `browseVignettes("ropenaq")` to get more documentation on `{ropenaq}` and what you can do with this package and data. 

**<mark>At time of writing the server seems to be down</mark>**
So the code below does not work at the moment.

```{r, eval=FALSE}
## https://github.com/ropensci/ropenaq
# devtools::install_github("ropensci/ropenaq")
library(ropenaq)
library(knitr)

## get countries
countries_table <- aq_countries()
kable(countries_table)

# get metadata on the query (important for reproducubility)
attr(countries_table, "meta")
attr(countries_table, "timestamp")

cities_table_mx <- aq_cities(country="MX")
kable(cities_table_mx)

locations_table_mx <- aq_locations(country = "MX", city = "MEXICO+STATE", limit = 10, page = 1)
kable(locations_table_mx)

## only locations that have data for paramter pm25
all_pm25_mx <- locations_table_mx %>%
  dplyr::filter(pm25 == TRUE)

locations <- unique(all_pm25_mx$locationURL)
cities <- unique(all_pm25_mx$cityURL)

## get data for one location
results_table <- aq_latest(country = "MX", location = locations[2])
kable(results_table)
```

### <mark>**If you are looking for a dataset to use for your capstone project, the OpenAQ dataset is a good candidate**</mark> {-}

### <mark>**EXERCISE 9; OpenAQ data (Airquality)**</mark> {-}

A) Which cities of the Netherlands are represented in the data

B) If you want to create maps from the OpenAQ data, where would you look for information on how to do this (in R)? Can you find and run a good example?

## Dedicated API's
The Open Data API from the Centraal Bureau voor de Statistiek in the Netherlands is a good example of a well-documented API that we can use to get valuable datasets. During the course a demo will be provided on the use of this API/portal.

## Scraping of websites

Climbing accidents
```{r}
library(rvest)
library(tidyverse)

url_accidents <- read_html("https://www.klimongevallen.nl/ongevallen/")

accidents_page <- url_accidents %>%
  html_nodes("#ongevallentable") 

table <- accidents_page[[1]] %>%
  html_table() %>%
as_tibble()

table
names(table)

ind <- str_detect(string = table$Letsel, 
           pattern = regex("overleden", ignore_case = TRUE))

deaths <- table[ind, ]

deaths 


ind <- str_detect(string = table$Letsel, 
                  pattern = regex("zwaar gewond", ignore_case = TRUE))


severe <- table[ind,]


deaths_severe <- dplyr::bind_rows(deaths, severe) 
deaths_severe %>%
  mutate(datum_new = lubridate::ymd(Datum)) %>%
  arrange(desc(datum_new)) %>%
  select(Datum, Activiteit, Locatie, Letsel) %>% knitr::kable()

injuries <- unique(table$Letsel) %>%
  enframe()


table_new <- table %>%
  mutate(Datum_new = lubridate::ymd(Datum)) %>%
  mutate(year = lubridate::floor_date(Datum_new, unit = "years")) %>%
  mutate(letsel_new = 
           str_replace_all(string = Letsel, pattern = regex("Geen.|Geen"),
                           replacement = NA_character_)) %>%
  na.omit()


stats <- table_new %>%
#  dplyr::filter() %>%
  group_by(Activiteit, year) %>%
  tally() %>%
  arrange(desc(n))
 

plot1 <- stats %>%
  ggplot(aes(x = year, y = n)) +
  geom_point(aes(colour = Activiteit), size = 3) +
  geom_smooth(aes(group = Activiteit,
                  colour = Activiteit), 
              method = "lm") +
#  citrulliner::theme_individual() +
  ggtitle("Gerapporteerd ongelukken per jaar")

ggsave(plot = plot1, filename = "years.svg",
       width = 18, height = 8, dpi = 300)
ggsave(plot = plot1, filename = "years.png",
       width = 18, height = 8, dpi = 300)

stats <- table %>%
  #  dplyr::filter() %>%
  group_by(Activiteit, Letsel) %>%
  tally() %>%
  arrange(desc(n))

stats

plot_def <- stats %>%
arrange(desc(n)) %>%
  na.omit() %>%
#  dplyr::filter(n > 1) %>%
  mutate(letsel_trunc = str_trunc(string = Letsel, width = 12)) %>%
  ggplot(aes(x = letsel_trunc, y = n)) +
  geom_point(aes(colour = Activiteit), position = "jitter") +
  coord_flip()

plot_def
```


### <mark>**EXERCISE 10; CBS - Open Data**</mark> {-}

https://www.cbs.nl/nl-nl/onze-diensten/open-data/statline-als-open-data/snelstartgids

### <mark>**If you are looking for a source to find dataset to use for your capstone project, the CBS Open Data API/Platform is a good place to look**</mark> {-}

A. Follow the installation instrunctions for the R package `{cbsopendataR}`
```{r}
# install.packages("cbsodataR")
library(cbsodataR)
```

B. Print a table (to an Excel file, containing all avaiable datasets). Use a pipe, `{readxl}` and the function `cbs_get_toc()`
```{r}
toc <- cbs_get_toc()
head(toc)

toc$ShortTitle[1:10]

# Downloaden van gehele tabel (kan een halve minuut duren)
data <- cbs_get_data("83765NED")
head(data)

# Downloaden van metadata
metadata <- cbs_get_meta("83765NED")

typeof(metadata)
metadata$DataProperties
head(metadata$TableInfos %>% as_tibble())

```

### <mark>**EXERCISE 11; Download a dataset via `{rdryad}`**</mark> {-}

*PRACTICE FOR CAPSTONE*

https://github.com/ropensci/rdryad

Create 4 valuable visualizations for this data. You may freely choose your dataset and the visualizations as long as you formulate a relevant research (complete the full PPDAC cycle). This is good practice for the capstone, but do not spend too much time... You may send a rendered Rmd to HTML file via the email to me to get feedback. 
Good luck!
