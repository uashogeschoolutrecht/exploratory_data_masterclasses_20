# Lab 5; Exploratory Data Analysis {#lab5eda}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 3)
```

```{r, root_1, include=FALSE}
## defines the root of the project for later use
require("rprojroot") || utils::install.packages("rprojroot")
library(rprojroot)
root <- find_root_file(criterion = is_rstudio_project)
```

## Packages
```{r}
library(tidyverse)
library(naniar)
library(ggplot2)
```

## Exploratory Data Analysis

 * Use visualisation and transformations to explore your data in a systematic way
 * A task that statisticians call exploratory data analysis, or EDA for short. 
 
## EDA is an iterative cycle; you:

 1) Generate questions about your data.
 2) Search for answers by visualising, transforming, and modelling your data.
 3) Use what you learn to refine your questions and/or generate new questions.

__You do not need to know statistics for EDA, but it helps if you do!__

## EDA is not a formal process with a strict set of rules. 

 * EDA is a state of mind. 
 * Should feel free to investigate every idea that occurs to you. 
 * Some of these ideas will pan out, and some will be dead ends. 
 * As your exploration continues, you will home in on a few particularly productive areas that you'll eventually write up and communicate to others.

## EDA Steps
We already saw the 'non-formal' EDA steps (PPDAC cycle) in the EDA case example in Chapter \@ref(edacase)

 1. Start with a **PROBLEM** (or question)
 1. You devise a **PLAN** to solve the problem (or answer the question)
 1. You collect **DATA** to execute the plan (or perform an experiment)
 1. You **ANALYZE** the data
 1. Finally, you draw a **CONCLUSION** from the analysis
 1. You probably start the cycle again

## EDA and R programming
```{r, echo=FALSE, fig.align='left'}
path_to_image <- file.path(root, "images", "data-science.png")
knitr::include_graphics(path_to_image, dpi = 60)
```

To do data analysis, you'll need to deploy all the tools of EDA: visualisation, transformation, and modelling.

## Prerequisites

In this lesson we'll combine what you've learned about dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions.

```{r, packages, message = FALSE}
library(tidyverse)
```

## When perfoming EDA consider:

 1. What question(s) are you trying to solve (or prove wrong)?
 1. Which information do you need and can you come up with a plan to answer the question(s)
 1. What kind of data do you have and how do you treat different types?
 1. What’s missing from the data and how do you deal with it?
 1. Where are the outliers and why should you care about them?
 1. How can you add, change or remove features to get more out of your data?
 1. Do you need additional data from other sources to relate to the dataset under scrutany?
 1. Are underlying statitical assumptions met / how is data distribution looking?
 1. What (exploratory) models apply or fit well to the data?
 1. What is the undelying (experimental) design?
 1. Is there multi-colinearity?
 
We will address each of these questions with an example dataset in the demo that follows.
 
## Definitions

 * A __variable__ is a quantity, quality, or property that you can measure. 
 * A __value__ is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.
 * An __observation__ is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. I'll sometimes refer to an observation as a data point.
 * Tables: __Tabular data__ is a set of values, each associated with a variable and an observation. 
 * Tabular data is _tidy_ if each value is placed in its own "cell", each variable in its own column, and each observation in its own row. 
 * In real-life, most data isn't tidy, as we've seen in __tidy data__.

## Variation

**Variation** is the tendency of the values of a variable to change from measurement to measurement. 

 * Categorical variables can also vary if you measure across different subjects (e.g. the eye colors of different people), or different times (e.g. the energy levels of an electron at different moments). 
 
 * Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable's values.

## Categorical variables

 * A variable is **categorical** if it can only take one of a small set of values.   
 * In R, categorical variables are usually saved as factors or character vectors. 
 * To examine the distribution of a categorical variable, use a bar chart:

## Demo from "R for Data Science" by Hadley Wickham & Garret Grolemund 

This demo was partly reproduced from the R4DS book by Grolemund and Wickham and involves an example on how to answer questions using EDA on the `diamonds` dataset that is an example dataset and part of the R-package `{ggplot2}`

### General questions?

 1. Which information do you need and can you come up with a plan to answer the question(s) (PP)
 1. What kind of data do you have and how do you treat different types? (D)

Here we already have the data: The infamous 'diamonds' dataset, build into the `{ggplot2}` package

The description of the data can be reviewed with
```{r}
# ?ggplot2::diamonds
```

From this description we could come up with a rather simple research question:
"Is `carat`, the weight of a diamond, correlated with the `price` of a diamond." Below we will address this question.

## Missingness & Outliers 

 1. What’s missing from the data and how do you deal with it?
 1. Where are the outliers and why should you care about them?

Are there any missing data in the diamonds dataset?
```{r}
data("diamonds")
naniar::vis_miss(diamonds)
sum(is.na(diamonds))
```

There seem to be no apparant missingness in any of the variables in the diamonds dataset. 

## EXERCISE 1: "Missingness" {-} 
Checking for missingness is a crucial first step to avoid problems later in the analysis (for example when deriving summary statistics)

 A) List two potential problems that can occur if missingness is not properly inspected and subsequently dealt with
 B) Read this article: https://medium.com/coinmonks/dealing-with-missing-data-using-r-3ae428da2d17 Summarize the different types of missingness and how you could deal with them. Create a table in your Rmd file (see: https://rmarkdown.rstudio.com/lesson-7.html)

## --- END OF EXERCISE --- {-}


### Distributions: The bar chart
To get an idea on the binning of a categorical variable (how many observations belong to which category?)
```{r}
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))
```

### Count can also compute frequencies for categorical values 
```{r}
diamonds %>% 
  count(cut) ## the sort argument (set to TRUE will show the largest on top)
```

### Continuous variables
 
 * A variable is **continuous** if it can take any of an infinite set of ordered values. 
 * Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use a histogram:

### The histogram
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.05)
```

What do you notice from this plot?

### Frequency calculation by 'hand'
You can compute this by hand by combining `dplyr::count()` and `ggplot2::cut_width()`:
```{r}
diamonds %>% 
  count(cut_width(carat, 0.05))
```

### Experiment with `binwidth =` 

 * Always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. 

```{r}
smaller <- diamonds %>% 
  dplyr::filter(carat < 3)
  
smaller %>%
  ggplot(aes(x = carat)) +
    geom_histogram(binwidth = 0.1)
```

### Multiple histograms in one plot: `geom_freqpoly()`
```{r}
smaller %>%
  ggplot(aes(x = carat, colour = cut)) +
    geom_freqpoly(binwidth = 0.1) +
    theme_bw() ## set to bw for better contrast
```

### Subsequent questions
 
 * Rely on your curiosity (What do you want to learn more about?) 
 * As well as your skepticism (How could this be misleading? Does something seem odd?)

### Typical values

look for anything unexpected:

* Which values are the most common? Why?
* Which values are rare? Why? Does that match your expectations?
* Can you see any unusual patterns? What might explain them?

### This plot can be explored with a couple of questions
```{r}
ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.01)
```

### Exploratory questions

* Why are there more diamonds at whole carats and common fractions of carats?
* Why are there more diamonds slightly to the right of each peak than there 
  are slightly to the left of each peak?
* Why are there no diamonds bigger than 3 carats?

### Understanding clusters in your data

* How are the observations within each cluster similar to each other?
* How are the observations in separate clusters different from each other?
* How can you explain or describe the clusters?
* Why might the appearance of clusters be misleading?

### Generate a plot that shows clusters takes experimentation and practice
This is the famous data from the 'faithful' dataset, showing timings of eruptions of the "Old Faithful" Geyser in Yellowstone National Park, USA
```{r}
ggplot(data = faithful, mapping = aes(x = eruptions)) + 
  geom_histogram(binwidth = 0.25)

## ?faithful for more details on the data
```  

### Unusual values or outliers

 * Outliers are observations that are unusual; 
 * Data points that don't seem to fit the pattern. 
 * Sometimes outliers are data entry errors; 
 * Other times outliers suggest important new science.
 * Outliers are sometimes difficult to see in a histogram.  

## Histogram to spot outliers
When you see a histogram with an extraordinary large x-axis, you have to be vigilant about outliers 
```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)
```   

### Zooming in on your data

To make it easy to see the unusual values, we need to zoom into to small values of the y-axis with `coord_cartesian()`:
This truncates the y-axis
```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 50))
```   

### Zoomed in
This allows us to see that there are three unusual values: 0, ~30, and ~60. We pluck them out with dplyr: 

```{r, include = FALSE}
## this chunk sets the print option for nice-tibble printing
old <- options(tibble.print_max = 10, tibble.print_min = 10)
```

```{r}
unusual <- diamonds %>% 
  dplyr::filter(y < 3 | y > 20) %>% 
  arrange(y)
unusual
```

### Do not throw away your data ligthly

 * Repeat your analysis with and without the outliers
 * If they have minimal effect on the results, and you can't figure out why they're there, replace them with missing values
 * If they have a substantial effect on your results, you shouldn't drop them without justification
 * Figure out what caused them (e.g. a data entry error) and disclose that you removed them in your report.

### Boxplot
```{r}
diamonds %>%
  ggplot(aes(x = cut, y = y)) +
  geom_boxplot()
```

## Missing values

If you've encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.

Drop the entire row with the strange values:
```{r, eval = FALSE}
diamonds2 <- diamonds %>% 
  dplyr::filter(between(y, 3, 20))
```
    
I don't recommend this option

## Better!

Replacing the unusual values with missing values.

### **EXERCISE 2: Replace outliers for missing values**

A) Replace the values smaller than 3 and larger than 20 for missing values
```{r, include=FALSE}
  diamonds2 <- diamonds %>% 
      mutate(y = ifelse(y < 3 | y > 20, NA, y))
```

## Missing values warning in ggplot2
Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. It's not obvious where you should plot missing values, so ggplot2 doesn't include them in the plot, but it does warn that they've been removed:

B) Plot the x and y variables of the `diamonds2` dataset that you have created above in a scatter plot, write down the warning message.

```{r, include=FALSE}
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point()
```

C) What is bad about the plot below?
List a few points and try to correct the plot (there are a number of ways that you could solve this).
```{r, echo=TRUE}
nycflights13::flights %>% 
  mutate(cancelled = is.na(dep_time),
         sched_hour = sched_dep_time %/% 100,
         sched_min = sched_dep_time %% 100,
         sched_dep_time = sched_hour + sched_min / 60) %>%
  ggplot(mapping = aes(sched_dep_time)) + 
  geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4) 
```

```{r, include=FALSE}
## one possible answer
nycflights13::flights %>% 
  mutate(cancelled = is.na(dep_time),
         sched_hour = sched_dep_time %/% 100,
         sched_min = sched_dep_time %% 100,
         sched_dep_time = sched_hour + sched_min / 60) %>%
  ggplot(mapping = aes(sched_dep_time)) + 
  geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4) +
  facet_wrap(~cancelled, scales = 'free')
```

## Covariation

 * Variation describes the behavior _within_ a variable
 * Covariation describes the behavior _between_ variables. 
 * **Covariation** is the tendency for the values of two or more variables to vary together in a related way. 
 * Best way to spot covariation is to visualise the relationship between two or more variables. 
 * How you do that should again depend on the type of variables involved.

### **EXERCISE 3: A categorical and continuous variable**

A) Create a plot showing the relationship between `price` and `cut` of the `diamonds` data. Use a frequency polynom to display the counts in each level for `cut` 

```{r, include=FALSE}
ggplot(data = diamonds, mapping = aes(x = price)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

It's hard to see the difference in distribution because the overall counts differ so much:

B) Create a bar graph displying the same information as in A) above. Did this improve the comparison?
```{r, include=FALSE}
ggplot(diamonds) + 
  geom_bar(mapping = aes(x = cut))
```

C) Create the same plot as in A) but now use `density` in stead of counts on the y-axis. Google for a solution if necessary.
```{r, include=FALSE}
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```


## The boxplot
A **boxplot** is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of:

```{r, echo = FALSE}
path_to_image <- file.path(
  here::here(
  "images", 
  "EDA-boxplot.png"))
knitr::include_graphics(path = path_to_image, dpi = 250)
```

## **EXERCISE 4: Diamonds boxplot

A) Create a boxplot for the distribution of price by cut using `geom_boxplot()`

```{r, include=FALSE}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()
```

B) Now log10 transform the `price` variable, what happens?
```{r, include=FALSE}
ggplot(data = diamonds, mapping = aes(x = cut, y = log10(price))) +
  geom_boxplot()
```

## Factors or "grouping"  variables

`cut` is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don't have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is with the `reorder()` function.

## Reorder the factor levels
For example, take the `class` variable in the `mpg` dataset. You might be interested to know how highway mileage varies across classes:

```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) +
  geom_boxplot()
```

## **EXERCISE: Reordering by continuous variable**

A) To make the trend easier to see, reorder by `class` based on the median value of `hwy`:

```{r, include=FALSE}
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), 
                             y = hwy)) + xlab("Class")
```

## Positioning labels on axis, or flip the axis
If you have long variable names, `geom_boxplot()` will work better if you flip it 90°. You can do that with `coord_flip()`.

```{r}
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip() + xlab("Class")
```

## Two categorical variables
To visualise the covariation between categorical variables, you'll need to count the number of observations for each combination. One way to do that is to rely on the built-in `geom_count()`:

```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))
```

## Circles

* The size of each circle in the plot displays how many observations occurred at each combination of values. 

* Covariation will appear as a strong correlation between specific x values and specific y values. 

## Two continuous variables
```{r, dev = "png"}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = carat, y = price))
```

## Fix overplotting by setting transparency
You've already seen one way to fix the problem: using the `alpha` aesthetic to add transparency.

```{r, dev = "png"}
ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price), alpha = 3 / 100) +
  geom_smooth(aes(x = carat, y = price), method = "lm", fullrange = FALSE )
```

## Fixing overplotting with binning
```{r}
# install.packages("hexbin")
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))
```

## Use bin to one continuous variable
Another option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. For example, you could bin `carat` and then for each group, display a boxplot:

```{r}
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))
```

## Now we can visualise the relationship between price and carat more clearly

```{r}
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1))) +
  geom_smooth(method = "auto")

```

## Methods of modelling

 * This is no statitics course, but:
 * Models are important
 * Look at `?geom_smooth`
 * The `method` argument has several options
 * The graph above shows a smoother with the model `method = "gam"`
 * More methods are available
 * See what happens if you change `method` to "lm"
 * Models help you understand relationships and are food for further formal statistical inference.
 * We could ask: 
 __"Is the relationships between price and carat really sigmoidal"?__

## cut_number
```{r}
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))
```


## EXERCISES

## Data
The dataset that we are going to use for these exercises here is the 'household_power_consumption' dataset.
The data can be downloaded from here:
https://github.com/rdpeng/ExData_Plotting1 

The information about this dataset can also be found here: https://www.kaggle.com/uciml/electric-power-consumption-data-set

We will concentrate on the first three months of the year 2006, 2007, 2008.

## EXERCISE 1: Power consumption {-}

**TIPS**

 - You probable need to filter and aggregate the data first before you can answer the questions below
 - First create quick and dirty graphs, later refine your code and you graph
 

 A) Execute the following steps in a code chunk:
 
 - Unzip the downloaded energy consumption file
 - Read the file into R, you should get a dataframe with 2,075,259 rows and 

```{r, include=FALSE}
download.file("https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2Fhousehold_power_consumption.zip", destfile = here::here(
    "data",
    "household_power_consumption.zip"
))
unzip(
  here::here(
    "data",
    "household_power_consumption.zip"), 
    exdir = here::here("data", "household_powerconsumption.txt"))

data_energy <- readr::read_csv(
  here::here(
    "data",
    "household_power_consumption.zip")
)
data_energy
```
 
B) Check if there are missing values anywhere in the data

```{r, include=FALSE}
data_energy %>%
  is.na %>%
  sum()




```

C) Create a visualization of the variables `Global_active_power` and `Date` that is meaningful and shows a clear pattern of power consumption of the year 2008

D) Show a number of visualization that show the relationship between the time of day and power consumption

E) Did power consumption over rhe years 2006, 2007 and 2008 show difference in patterns over the months? 

## EXERCISE 2: Missingness {-}

A) Load the dataset called `riskfactors` into your R session

```{r, include=FALSE}
data("riskfactors", package = "naniar")

```

B) Inspect the missingness in this data, use the naniar package to get an idea on where the missingness is located.

```{r, include=FALSE}
naniar::vis_miss(riskfactors)
```

C) Which variables would you consider removing from the dataset on the basis of their missingness?

D) Write a function that calculates the percentage of missingness for a each variable in a dataset

E) Use this function to filter each variable that has a missingness that exceeds 80%, use this rule to generate a new version of the data with these variable (>80% mssingness) removed. 

F) Create a number of visualization to show the relationship between the variable `health_general` and a number of other variables that you think are relevant to investigate.

```{r}
riskfactors %>%
  ggplot(aes(x = drink_average, y = health_general)) +
  geom_point(aes(colour = sex), position = "jitter")

riskfactors %>%
  ggplot(aes(x = smoke_days, y = health_general)) +
  geom_point(aes(colour = sex), position = "jitter")


```

As you see, it might prove really difficult to tell anything about risk factors when we have so little data. Here we typically have a dataset that relative large P (predictors) and relatively small N (number of observations). In the next EXERCISE, we will use a different dataset that has a much larger N. This problem of large P, small N is common for Biology or Health Science related data.

## EXERCISE 3; Cocktails {-}

A) Load the cocktails dataset from here https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv into R as `cocktails`

```{r, answer_a, include=FALSE}
# Code for quick exploration of
# https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-05-26
# Video at https://youtu.be/kHFmtKCI_F4


library(tidyverse)

cocktails <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv")

```

B) For this exercises we focus on the `ingredient` variable. How many unique ingredients are included in the `cocktails` dataset 

```{r, answer_b}
cocktails$ingredient %>%
  unique() %>% 
  length()
```


C) Retrieve the top 30 most common ingredients

```{r, include = FALSE}
## this chunk sets the print option for nice-tibble printing
options(tibble.print_max = 30, tibble.print_min = 10)
```

```{r}

top_30 <- cocktails %>%
  count(ingredient, sort = TRUE) %>%
  head(30)
top_30

## restore setting for tibble printing
options(tibble.print_max = 10, tibble.print_min = 10)

```

C) The data shows a `name` and a `row_id` variable. Are `name` and `row_id` equivalent? 

```{r}
cocktails %>% count(name)
cocktails %>% count(row_id)

cocktails %>%
  group_by(name) %>%
  summarise(ids = n_distinct(row_id)) %>%
  filter(ids > 1)
# Yes, they are

```


D) How many ingredients are in each cocktail? ------------------------------
```{r}
cocktails %>%
  count(name) %>%
  count(n)

# These 1-ingredient cocktails suggest that non-alcoholic ingredients
# are not included
cocktails %>%
  group_by(name) %>%
  filter(n() == 1)

cocktails %>%
  group_by(name) %>%
  filter(n() == 2)
```

E) How big (`measure`) is each cocktail? 

```{r}
cocktails %>% count(measure, sort = TRUE)

non_spirit <- c("Chilled Champagne", "Water", "Orange Juice", "Cranberry Juice", "Light Cream (if desired)", "Fresh orange juice", "Orange juice")
```


F) Correct the `measure` variable in an SI unit (milliliters) 
# Have removed all cocktails with ounces of bitters - that is most
# likely a data entry error (it certainly doesn't give me much confidence
# in the quality of this data)

sizes <- cocktails %>%
  filter(str_detect(measure, "oz")) %>%
  filter(!str_detect(ingredient, fixed("bitters", ignore_case = TRUE))) %>%
  filter(!ingredient %in% non_spirit) %>%
  mutate(oz = str_replace(measure, " oz", "")) %>%
  mutate(oz = str_replace(oz, " ?1/2", ".5")) %>%
  mutate(oz = str_replace(oz, " ?1/4", ".25")) %>%
  mutate(oz = str_replace(oz, " ? ?3/4", ".75")) %>%
  mutate(oz = str_replace(oz, " ?1/3", ".33")) %>%
  mutate(oz = str_replace(oz, " ?2/3", ".33")) %>%
  mutate(oz = as.numeric(oz))

filter(sizes, oz > 3)
filter(sizes, oz > 10)

total_size <- sizes %>% group_by(name) %>% summarise(n = n(), oz = sum(oz))

total_size %>% filter(oz > 20)

total_size %>%
  filter(oz < 20) %>%
  ggplot(aes(oz)) +
  geom_histogram(binwidth = 0.5)

total_size %>%
  filter(oz > 6) %>%
  semi_join(cocktails, ., by = "name") 

cocktails %>%
  filter(str_detect(ingredient, "bitters"))

sizes %>%
  group_by(ingredient) %>%
  summarise(n = n(), oz = mean(oz)) %>%
  filter(n > 5) %>%
  arrange(desc(oz), sort = TRUE)


# What are the primary ingredients ----------------------------------------

cocktails <- cocktails %>% mutate(ingredient = tolower(ingredient))

cocktails %>%
  count(ingredient = tolower(ingredient), sort = TRUE) %>%
  head(20)

standard_ingredients <- tribble(
  ~ ingredient,        ~ standard_name,
  "fresh lemon juice", "lemon juice",
  "juice of a lemon",  "lemon juice",
  "fresh lime juice",  "lime juice",
  "juice of a lime",   "lime juice",
  "bitters",           "angostura bitters"
)

ingredient_changes <- cocktails %>%
  select(name, ingredient_number, ingredient) %>%
  right_join(standard_ingredients) %>%
  select(name, ingredient_number, ingredient = standard_name)

cocktails %>%
  rows_update(ingredient_changes, by = c("name", "ingredient_number")) %>%
  count(ingredient, sort = TRUE) %>%
  head(20)
```

 

 ## Case study
 
 https://data-analist.info/cbs-open-data-downloaden-voor-data-analyse-in-r/

 

 






